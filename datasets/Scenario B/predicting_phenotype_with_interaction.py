# -*- coding: utf-8 -*-
"""predicting phenotype with interaction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/19CitI4fzEn7PRHiIGqYkUdGqMgxonagv
"""

from google.colab import drive
drive.mount('/content/drive')

##import library
import gc
import os
os.chdir('/content/drive/MyDrive/Colab Notebooks/GitHub')
import numpy as np
import networkx
import csv
import pandas as pd
from tqdm import tqdm
import gzip
import re
import matplotlib.pyplot as plt
import numpy as np
from sklearn.linear_model import LogisticRegression
from metrics import metrics
from labelling import label_ADNI
from sklearn.model_selection import train_test_split
from sklearn.model_selection import StratifiedKFold
import itertools
from sklearn.metrics import log_loss
import random
from sklearn.metrics import mean_squared_error
import pickle

# https://realpython.com/logistic-regression-python/

# Percorso del file GZIP
file_path = '/content/drive/MyDrive/PROJECT/chr_19_no_NaN.csv.gz'

dataset = pd.read_csv(file_path, sep='\t', dtype=str)
dataset.set_index('Unnamed: 0', inplace=True)
dataset = dataset.astype(float)
dataset

# Percorso del file GZIP
file_path = '/content/drive/MyDrive/PROJECT/lookup_snpid_gene.csv.gz'

gene_snp = pd.read_csv(file_path, sep='\t', dtype=str)
gene_snp[gene_snp['gene_name'] == 'None'] = np.nan
gene_snp = gene_snp.set_index('snpid')
gene_snp = gene_snp.drop('Unnamed: 0', axis = 1)
gene_snp = gene_snp.dropna()
usefull_snps = dataset.columns.tolist()[:-3]
gene_snp = gene_snp.loc[usefull_snps]
gene_snp

lista_APOE = gene_snp[gene_snp['gene_name'] == 'APOE'].index.tolist()
lista_APOB = gene_snp[gene_snp['gene_name'] == 'APOB'].index.tolist()
lista_LDLR = gene_snp[gene_snp['gene_name'] == 'LDLR'].index.tolist()
lista_interactions = [f'{a}_{b}' for a, b in itertools.product(lista_APOE, lista_APOB)] + [f'{a}_{b}' for a, b in itertools.product(lista_APOE, lista_LDLR)] + [f'{a}_{b}' for a, b in itertools.product(lista_LDLR, lista_APOB)]

#remove current phenotype
dataset = dataset.drop(['y'], axis = 1)
# create a dataframe with interaction columns
df = pd.DataFrame(columns = lista_interactions)
#concatenate it to the dataset
dataset_2 = pd.concat([dataset, df], axis=1)
#create vector with index of APOE gene and age
dataset_2

index = np.where(np.isin(dataset_2.columns, lista_interactions))
#index = np.append(index, len(dataset_2.columns)-1)
# build the coefficients of log reg to have a 60% prob to have AD with all the snps = 1 and age = 0 and 90 % with all at1 + age = 1
weights = np.zeros([1,len(dataset_2.columns)])
weights[0, index] = 0.02
#weights[0, index[0]-1] = 1.5
model = LogisticRegression(max_iter=1000)
model.coef_ = weights
model.intercept_ = np.array([-2])
model.classes_ = np.array([0, 1])
# Store the coefficients in the dictionary using the column name as the key
coefficients = model.coef_
print(coefficients)

age = np.arange(0, 1.01, 0.01)
list_prob = []
vls = [0, 0.5, 1]
for j in vls:
  probability = []
  for i in age:
    x = np.zeros([1,len(dataset_2.columns)])
    x[:,len(dataset.columns)-1] = i
    x[:,index] = j
    x = x*i
    probability.append(model.predict_proba(x)[:, 1])
  list_prob.append(probability)
#print(x)
# Creazione del plot
plt.plot(age, np.squeeze(list_prob[0]), label='SNPs =  0')
plt.plot(age, np.squeeze(list_prob[1]), label='SNPs =  0.5')
plt.plot(age, np.squeeze(list_prob[2]), label='SNPs = 1')


# Aggiunta di etichette per gli assi x e y
plt.xlabel('Age')
plt.ylabel('Probability')

# Aggiunta di una legenda
plt.legend()
plt.title('Probability of being AD-positive in function of age')

# Mostrare il plot
plt.show()

#compute the probability to have a 0.5/0/1 in each snps based on the real data
lista_snps = lista_APOE + lista_APOB + lista_LDLR
index_important_snps = np.where(np.isin(dataset.columns, lista_snps))[0]
row, cols = dataset.values.shape
prob_0 = []
prob_0_5 = []
prob_1 = []
for i in range(cols-2):
  if i in index_important_snps:
    prob_0.append(0.1)
    prob_0_5.append(0.1)
    prob_1.append(0.8)
  else:
    snp = dataset.values.astype(float)[:,i]
    prob_0.append(np.count_nonzero(snp == 0)/row)
    prob_0_5.append(np.count_nonzero(snp == 0.5)/row)
    prob_1.append(np.count_nonzero(snp == 1)/row)
#set the important snps to fixed probability to increase the probability to have positive subjects

class_1 = pd.DataFrame(columns=[*dataset.columns, 'y'])
class_0 = pd.DataFrame(columns=[*dataset.columns, 'y'])
N_pos = 0
N_neg = 0
ne1 = 0
ne0 = 0
ng = 0
n = 0
n_2 = 0

while N_pos != 500 or N_neg != 500:
    new_element = np.array([])

    for i in range(cols-2):
        # Assign a value to each SNP based on probabilities
        value = np.random.choice([0, 0.5, 1], p=[prob_0[i], prob_0_5[i], prob_1[i]])
        new_element = np.append(new_element, value)

    # Simulate gender as a binary variable (1 = male, 0 = female)
    gender = np.random.choice([0, 1], p=[0.5, 0.5])
    new_element = np.append(new_element, gender)

    # Simulate age as a random number between 0 and 1
    age = random.uniform(0, 1)
    new_element = np.append(new_element, age)

    # Now add the values for the interaction
    new_element = np.reshape(new_element, (1, len(new_element)))


    for interaction in lista_interactions:
        snp1, snp2 = interaction.split('_')
        index1 = np.where(np.isin(dataset.columns, snp1))[0][0]
        index2 = np.where(np.isin(dataset.columns, snp2))[0][0]
        interaction_values = new_element[:, index1] * new_element[:, index2] * age
        new_element = np.hstack((new_element, interaction_values.reshape(-1, 1)))

    # Build the phenotype from the model
    prob = model.predict_proba(new_element)
    #print(prob)
    y = np.random.binomial(n=1, p=prob[0][1])
    #print(y)
    new_element = new_element[:,:-len(lista_interactions)]
    new_element = np.hstack((new_element, np.array(y).reshape(1, 1)))
    df = pd.DataFrame(new_element, columns=[*dataset.columns, 'y'])
    ng = ng+1

    if y == 1 and N_pos < 500:
      class_1 = pd.concat([class_1, df], axis=0).reset_index(drop=True)
      N_pos = N_pos+1
      n = n+1
      if prob[0][1] < 0.5:
        ne0 = ne0 + 1

    elif y == 0 and N_neg < 500:
      class_0 = pd.concat([class_0, df], axis=0).reset_index(drop=True)
      N_neg = N_neg+1
      n_2 = n_2+1
      if prob[0][1] > 0.5:
        ne1 = ne1 + 1


print('ne1 = ' + str(ne1))
print('ne0 = ' + str(ne0))
class_1

lung_datasets = [250, 500, 750, 1000]
list_datasets = []

for i in lung_datasets:
    lung = 0
    flag = 0
    indices = class_0.index.tolist()
    dataset_united = pd.DataFrame(columns=class_0.columns)

    while lung != i:
        if flag >= len(indices):
            break

        row_class_0 = class_0.iloc[indices[flag]].to_frame().T
        row_class_1 = class_1.iloc[indices[flag]].to_frame().T

        dataset_united = pd.concat([dataset_united, row_class_0], axis=0, ignore_index=True)
        dataset_united = pd.concat([dataset_united, row_class_1], axis=0, ignore_index=True)

        lung += 2
        flag += 1

    list_datasets.append(dataset_united)

list_datasets[3]

matrix = list_datasets[3].values
new_matrix = matrix[:, np.r_[:matrix.shape[1]-2, matrix.shape[1]-1]]

# Crea una nuova figura con dimensioni personalizzate
plt.figure(figsize=(15, 15))

# Visualizza la heatmap
plt.imshow(new_matrix, cmap='hot', interpolation='nearest')
plt.colorbar()

# Aggiungi titoli e etichette
plt.title("Heatmap - SNPs and Patients")
plt.xlabel("SNPs")
plt.ylabel("Patients")

# Mostra la heatmap ingrandita
plt.show()

# Calcola la media delle colonne
mean_vector = np.mean(list_datasets[3].values[:,:-3], axis=0)

# Ottieni l'indice del valore massimo nel vettore della media
max_index = np.argmax(mean_vector)
colonne = list_datasets[3].columns.tolist()
#colonne = colonne[:-3]
print('SNP: '+ str(colonne[max_index]))
print('probability to have a 0= '+str(prob_0[max_index]))
print('probability to have a 0.5= '+str(prob_0_5[max_index]))
print('probability to have a 1= '+str(prob_1[max_index]))
print('mean value = ' + str(mean_vector[max_index]))

print('gene: '+ gene_snp.loc[colonne[max_index]]['gene_name'])

with open('/content/drive/MyDrive/PROJECT/list_datasets_madeup_pheno_Age_Sex_interactions_no_processing.pickle', 'wb') as handle:
    pickle.dump(list_datasets, handle, protocol=pickle.HIGHEST_PROTOCOL)

# checkejem que s'obri correctament:
with open('/content/drive/MyDrive/PROJECT/list_datasets_madeup_pheno_Age_Sex_interactions_no_processing.pickle', 'rb') as handle:
    dataset_2 = pickle.load(handle)
dataset_2[3].shape



