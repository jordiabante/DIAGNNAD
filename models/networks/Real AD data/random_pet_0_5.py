# -*- coding: utf-8 -*-
"""Random_PET_0.5.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11RZeOngYU4VLbYzkrBMXx6_3Yq583dT1
"""

import os.path as osp
from math import ceil
import pickle
from sklearn.metrics import balanced_accuracy_score
from sklearn.manifold import TSNE

import torch
import torch.nn.functional as F
from torch.nn import Linear

from torch_geometric.datasets import TUDataset
from torch_geometric.loader import DataLoader
from torch_geometric.nn import DenseGraphConv, DMoNPooling, GCNConv, GATv2Conv, TopKPooling, global_mean_pool
import torch.nn as nn
from torch_geometric.utils import to_dense_adj, to_dense_batch
import matplotlib.pyplot as plt
import os
import numpy as np
import math
import json
from torch.optim.lr_scheduler import OneCycleLR
import torch.nn.functional as F
from sklearn.model_selection import ParameterGrid
import time
from torch_geometric.explain import Explainer, GNNExplainer
import random
import sys



sys.path.append('/biofisica/home/creatio_student/project_gnn_ad/file_articolo/')
from metrics import metrics
from gnn_definitions import train, validation, test, GCNNet
os.chdir('/biofisica/home/creatio_student/project_gnn_ad/models/Random_grid_0.5')

with open('/biofisica/home/creatio_student/project_gnn_ad/file_articolo/training_0.5_GNN_PET.pickle', 'rb') as handle:
    training_list = pickle.load(handle)

with open('/biofisica/home/creatio_student/project_gnn_ad/file_articolo/validation_0.5_GNN_PET.pickle', 'rb') as handle:
    validation_list = pickle.load(handle)

file1 = open('./winner_params.json', 'r')
winner_params = json.load(file1)

# Set the number of epochs
num_epoch = 2000
# set the batch size
batch_size = 16

start_time = time.time()
# Initialize lists to store the validation metrics for each trial
val_accuracy_list = []
val_loss_list = []
val_cm_list = []
val_balanced_accuracy_list = []
val_precision_list = []
val_recall_list = []
val_specificity_list = []
val_NPV_list = []
val_f1_list = []
val_roc_auc_list = []
val_thresholds_list = []
x_toplot_post_GNN = []
x_toplot_post_lin = []
embeddings = []
train_loss_list = []


# Loop over the specified number of trials
for i in range(len(training_list)):

    edge_index = training_list[0][0].edge_index
    num_nodes = training_list[0][0].num_nodes

    src_nodes = edge_index[0]
    dst_nodes = edge_index[1]

    edge_list = list(zip(src_nodes.tolist(), dst_nodes.tolist()))
    random.shuffle(edge_list)

    num_edges = len(edge_list)

    new_src_nodes = []
    new_dst_nodes = []
    new_edge_index = []

    for k in range(int(num_edges/2)):

        new_src = random.randint(0, num_nodes - 1)
        new_dst = random.randint(0, num_nodes - 1)

        new_edge_index.append([new_src,new_dst])
        new_edge_index.append([new_dst,new_src])

    new_edge_index = torch.tensor(new_edge_index, dtype=torch.long).t().contiguous()

    adj_matrix = torch.sparse_coo_tensor(new_edge_index, torch.ones(num_edges), size=(num_nodes, num_nodes))
    adj_matrix_dense = adj_matrix.to_dense()
    adj_matrix_dense[adj_matrix_dense != 0] = 1

    plt.imshow(adj_matrix_dense, cmap='hot', interpolation='nearest')
    plt.colorbar()
    plt.show()

    training_set = training_list[i]
    validation_set = validation_list[i]

    for j in training_set:
      j.edge_index = new_edge_index
    for j in validation_set:
      j.edge_index = new_edge_index

    # Create a DataLoader object for the validation set with batch size 16 and shuffle the data
    val_loader = DataLoader(validation_set, batch_size, shuffle=True)

    # Create a DataLoader object for the training set with batch size 16 and shuffle the data
    train_loader = DataLoader(training_set, batch_size, shuffle=True)

    # Initialize variables and set device to use GPU if available, CPU otherwise
    max_balanced = 0
    train_loss_list_epoch = []
    val_loss_list_epoch = []
    balanced_accuracy_list_epoch_val = []
    balanced_accuracy_list_epoch_train = []
    embeddings_2 = []
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = GCNNet(num_features=winner_params['num_features'],
               hidden_channels=winner_params['hidden_channels'],
               num_classes=winner_params['num_classes'],
               out_channels=winner_params['out_channels'],
               num_layer=winner_params['num_layer']).to(device)

    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    total_iterations = math.ceil(len(training_set) / batch_size)*num_epoch
    scheduler = OneCycleLR(optimizer, max_lr=0.01, total_steps=total_iterations)

    # Create a directory for each trial to save model checkpoints
    if not os.path.exists("./PET_iteration_"+ str (i+1)):
        os.mkdir("./PET_iteration_"+ str (i+1))
        os.chdir("./PET_iteration_"+ str (i+1))

    # Loop over the specified number of epochs
    for epoch in range(1, num_epoch+1):

        # Train the model
        train_loss = train(train_loader, model, device, optimizer, scheduler)

        # Calculate the training and validation loss and balanced accuracy
        train_loss_2, balanced_accuracy_train = validation(train_loader, model, device)
        val_loss,  balanced_accuracy = validation(val_loader, model ,device)

        # Print the current epoch's metrics
        print(f'trial: {i+1} Epoch: {epoch:03d}, Train Loss: {train_loss:.3f}, '
              f'Train Bal Acc: {balanced_accuracy_train:.3f}, Val Loss: {val_loss:.3f}, '
              f'Val Bal Acc: {balanced_accuracy:.3f}')

        # Append the current epoch's metrics to the corresponding lists
        balanced_accuracy_list_epoch_val.append(balanced_accuracy)
        balanced_accuracy_list_epoch_train.append(balanced_accuracy_train)
        train_loss_list_epoch.append(train_loss)
        val_loss_list_epoch.append(val_loss)

        # If the current epoch's balanced accuracy is higher than the previous highest, save the current model checkpoint
        if balanced_accuracy > max_balanced:
            max_balanced = balanced_accuracy
            torch.save(model.state_dict(), "./modello_" + "trial_" + str(i+1) + ".pt")
            print('model_saved at epoch ' + str(epoch) + ' with balanced_accuracy: ' + str(max_balanced))


    # Plot the loss and balanced accuracy for each epoch of the current trial
    plt.figure("train", (12, 6))
    plt.subplot(1, 2, 1)
    plt.title("Iteration Average Loss")
    x = [(i + 1) for i in range(len(train_loss_list_epoch))]
    y = train_loss_list_epoch
    y1 = val_loss_list_epoch
    plt.xlabel("Iteration")
    plt.plot(x, y,label="train BCE")
    plt.plot(x,y1,label="val BCE")
    plt.legend(loc="upper right")
    plt.subplot(1, 2, 2)
    plt.title("Val balanced Accuracy")
    x = [(i + 1) for i in range(len(balanced_accuracy_list_epoch_train))]
    y = balanced_accuracy_list_epoch_val
    y1 = balanced_accuracy_list_epoch_train
    plt.xlabel("Iteration")
    plt.plot(x, y,color='orange',label="val balanced Accuracy")
    plt.plot(x, y1,color='blue',label="train balanced Accuracy")
    plt.legend(loc="upper left")

    # Save the plot of the loss and balanced accuracy
    plt.savefig("./balanced_accuracy_loss.png")

    plt.show()
    plt.close()

    # Load the state dictionary of the model saved during training
    model.load_state_dict(torch.load("./modello_"+"trial_" + str(i+1) + ".pt"))

    # Test the model on the validation set
    val_loss, cm, accuracy, balanced_accuracy, precision, recall, specificity, NPV, f1, roc_auc, thresholds, x_full_test_post_GNN, x_full_test_post_lin  = test(val_loader, model, device)
    x_toplot_post_GNN.append (x_full_test_post_GNN)
    x_toplot_post_lin.append (x_full_test_post_lin)
    # Change the current working directory back to the original directory
    os.chdir('/biofisica/home/creatio_student/project_gnn_ad/models/Random_grid_0.5')

    # Append the evaluation metrics to their respective lists if they are not NaN
    train_loss_list.append(train_loss_list_epoch)
    val_cm_list.append(cm)
    if not math.isnan(accuracy):
      val_accuracy_list.append(accuracy)
    if not math.isnan(val_loss):
      val_loss_list.append(val_loss)
    if not math.isnan(balanced_accuracy):
      val_balanced_accuracy_list.append(balanced_accuracy)
    if not math.isnan(precision):
      val_precision_list.append(precision)
    if not math.isnan(recall):
      val_recall_list.append(recall)
    if not math.isnan(specificity):
      val_specificity_list.append(specificity)
    if not math.isnan(NPV):
      val_NPV_list.append(NPV)
    if not math.isnan(f1):
      val_f1_list.append(f1)
    if not math.isnan(roc_auc):
      val_roc_auc_list.append(roc_auc)
end_time = time.time()

result = {'accuracy': val_accuracy_list,
          'balanced_accuracy': val_balanced_accuracy_list,
          'recall' : val_recall_list,
          'precision': val_precision_list,
          'specificity': val_specificity_list,
          'NPV': val_NPV_list,
          'F1_score': val_f1_list,
          'AUC': val_roc_auc_list
}

os.chdir('/biofisica/home/creatio_student/project_gnn_ad/results/')
file_1 = open('./risultati_Random_PET_0.5.json','w')
json.dump(result,file_1)
file_1.close()

with open('/biofisica/home/creatio_student/project_gnn_ad/models/Random_grid_0.5/embeddings_post_GNN', 'wb') as file:
    pickle.dump(x_toplot_post_GNN, file)
with open('/biofisica/home/creatio_student/project_gnn_ad/models/Random_grid_0.5/embeddings_post_lin', 'wb') as file:
    pickle.dump(x_toplot_post_lin, file)

# Calcola il tempo trascorso
elapsed_time = end_time - start_time

# Stampa il tempo trascorso
print(f"Tempo trascorso: {elapsed_time} secondi")