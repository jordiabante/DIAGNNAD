# -*- coding: utf-8 -*-
"""GAT_madeup_pheno_Grid_search_0.75.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1J3gcIOkesDZD8iWHYw2D-akwE0od3-RC
"""

import os.path as osp
from math import ceil
import pickle
from sklearn.metrics import balanced_accuracy_score

import torch
import torch.nn.functional as F
from torch.nn import Linear

from torch_geometric.datasets import TUDataset
from torch_geometric.loader import DataLoader
from torch_geometric.nn import DenseGraphConv, DMoNPooling, GCNConv, TopKPooling, global_mean_pool
import torch.nn as nn
from torch_geometric.utils import to_dense_adj, to_dense_batch
import matplotlib.pyplot as plt
import os
import numpy as np
import math
import json
from torch.optim.lr_scheduler import OneCycleLR
from sklearn.model_selection import ParameterGrid
import time
import random
import sys


sys.path.append('/biofisica/home/creatio_student/project_gnn_ad/file_articolo/')
from metrics import metrics
from gnn_definitions import GAT, train, validation, test
os.chdir('/biofisica/home/creatio_student/project_gnn_ad/models/GAT_madeup_pheno_grid_0.75')

with open('/biofisica/home/creatio_student/project_gnn_ad/file_articolo/dataset_GNN_0.75_madeup_pheno.pickle', 'rb') as handle:
    dataset_list = pickle.load(handle)


# Set the number of epochs for the grid search
num_epoch_grid = 750
# set the batch size
batch_size = 16

# Define a list of values to search for each hyperparameter
start_time = time.time()
param_grid = {
    'num_features': [15],
    'hidden_channels': [8, 16, 32],
    'num_classes': [2],
    'out_channels': [1],
    'num_layer': [1, 2, 3], # Test different numbers of ConvGNN layers
}
max_balanced_step = []
# Generate all possible combinations of hyperparameters
param_list = list(ParameterGrid(param_grid))
j = 0
n = 3

# Loop through each combination of hyperparameters
for params in param_list:
    lista_balanced = []
    validation_list = []
    training_list = []
    n_elem = len(dataset_list[n])

    for i in range(5):
        if i == 4:
            tr = dataset_list[n][:int(n_elem/5*i)]
            vali = dataset_list[n][int(n_elem/5*i):]
        else:
            train1 = dataset_list[n][:int(n_elem/5*i)]
            train2 = dataset_list[n][int(n_elem/5*(i+1)):]
            tr = train1 + train2
            vali = dataset_list[n][int(n_elem/5*i):int(n_elem/5*(i+1))]

        validation_list.append(vali)
        training_list.append(tr)

        training_set = training_list[i]
        validation_set = validation_list[i]

        val_loader = DataLoader(validation_set, batch_size, shuffle=True)

        # Create a DataLoader object for the training set with batch size 16 and shuffle the data
        train_loader = DataLoader(training_set, batch_size, shuffle=True)

        print('dataset: ' + str(i+1) + ' ' + str(j+1) + ' parameters set')
        # Initialize the model with the current set of hyperparameters
        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        model = GAT(**params).to(device)


        # Define your optimizer (e.g., Adam optimizer with a specific learning rate)
        optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
        total_iterations = math.ceil(len(training_set) / batch_size)*num_epoch_grid
        scheduler = OneCycleLR(optimizer, max_lr=0.01, total_steps=total_iterations)

        # Define your loss function (e.g., binary cross-entropy loss for binary classification)
        loss_function = torch.nn.BCEWithLogitsLoss()
        max_balanced = 0
        for epoch in range(1, num_epoch_grid+1):

                # Train the model
                train_loss = train(train_loader, model, device, optimizer, scheduler)

                # Calculate the training and validation loss and balanced accuracy
                train_loss_2, balanced_accuracy_train = validation(train_loader, model, device)
                val_loss,  balanced_accuracy = validation(val_loader, model ,device)

                # Print the current epoch's metrics
                print(f'Epoch: {epoch:03d}, Train Loss: {train_loss:.3f}, '
                      f'Train Bal Acc: {balanced_accuracy_train:.3f}, Val Loss: {val_loss:.3f}, '
                      f'Val Bal Acc: {balanced_accuracy:.3f}')



                # If the current epoch's balanced accuracy is higher than the previous highest, save the current model checkpoint
                if balanced_accuracy > max_balanced:
                    max_balanced = balanced_accuracy
                    print('new max with balanced accuracy = ' + str(max_balanced))
        lista_balanced.append(max_balanced)
    j += 1
    max_balanced_step.append(np.mean(lista_balanced))


# Record and compare the model performance (e.g., validation accuracy) for each set of hyperparameters
# and select the best set of hyperparameters based on the performance metric of interest.
indice_massimo = max_balanced_step.index(max(max_balanced_step))
winner_params = param_list[indice_massimo]
print('the winner parameters are: ')
print(winner_params)

file_1 = open('./winner_params.json','w')
json.dump(winner_params,file_1)
file_1.close()