# -*- coding: utf-8 -*-
"""lightGBM_PET.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D7tmsNyRukp21iZ-7Tiv7RcbBMXiYLMX
"""

from google.colab import drive
drive.mount('/content/drive')

##import library
import gc
import os
os.chdir('/content/drive/MyDrive/Colab Notebooks/GitHub')
import numpy as np
import networkx
import csv
import pandas as pd
from tqdm import tqdm
import gzip
import re
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from metrics import metrics
from labelling import label_ADNI
from sklearn.model_selection import train_test_split
from keras import backend as K
from sklearn.metrics import balanced_accuracy_score
from sklearn.model_selection import StratifiedKFold
import itertools
import math
import pickle
import json

import lightgbm as lgb
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import log_loss

# https://realpython.com/logistic-regression-python/

# checkejem que s'obri correctament:
with open('/content/drive/MyDrive/PROJECT/training_set_Age_Sex.pickle', 'rb') as handle:
    training = pickle.load(handle)

with open('/content/drive/MyDrive/PROJECT/validation_set_Age_Sex.pickle', 'rb') as handle:
    validation = pickle.load(handle)
validation[0]

# Crea las listas vacías para las métricas
cm_list = []
accuracy_list = []
balanced_accuracy_list = []
precision_list = []
recall_list = []
specificity_list = []
NPV_list = []
f1_list = []
roc_auc_list = []
loss_list = []

for i in range(len(training)):
    df_train = training[i]
    df_test = validation[i]
    print(i, "test SET", len(df_test))

    X_train = df_train.drop(columns=['y'])
    y_train = df_train['y']
    y_train = y_train.astype(float).round(1)

    X_test = df_test.drop(columns=['y'])
    y_test = df_test['y']
    y_test = y_test.astype(float).round(1)

# creem el model d'arbres per classificar
    # Define the parameters for the grid search
    parameters = {
            'num_leaves': [10, 20, 30],  # Test different values for num_leaves
            'min_data_in_leaf': [10, 15, 20],  # Test different values for min_data_in_leaf, numeros grandes para evitar el sobreajuste, al tener un dataset pequeño podría ocurrir
            'max_depth': [None, 1, 5, 10]  # Test different values for max_depth
            }

    # Create the base model
    base_model = lgb.LGBMClassifier()

    # Perform grid search
    grid_search = GridSearchCV(base_model, parameters, cv=5, scoring='balanced_accuracy', n_jobs=-1)
    grid_search.fit(X_train, y_train)

    # Get the best parameters and the best model
    best_params = grid_search.best_params_
    best_model = grid_search.best_estimator_

    # Print the best parameters found
    print("Best parameters found:", best_params) # {'max_depth': 7, 'min_data_in_leaf': 20, 'n_estimators': 478, 'num_leaves': 15}
#perquè no provem en generalitzar els parametres? i utilitzar els més bons que trobi

      # predict the results
    y_pred=best_model.predict(X_test)
    y_pred = y_pred.astype(float).round(1)
    y_proba = best_model.predict_proba(X_test)[:,1] #to obtain the predicted probabilities for each class in a classification problem.


    # Compute the metrics
    classes=['0.0', '1.0']
    cm, accuracy, balanced_accuracy, precision, recall, specificity, NPV, f1, roc_auc, thresholds = metrics(y_test, y_pred, y_proba, classes)
    print("Precisió del model (balanced_accuracy): {:.2f}".format(balanced_accuracy))

    # Compute the loss function
    loss = log_loss(y_test, y_proba, normalize=True, sample_weight=None, labels=[0.0, 1.0])


    # Store the data in the lists
    cm_list.append(cm)
    accuracy_list.append(accuracy)
    balanced_accuracy_list.append(balanced_accuracy)
    precision_list.append(precision)
    recall_list.append(recall)
    specificity_list.append(specificity)
    NPV_list.append(NPV)
    f1_list.append(f1)
    roc_auc_list.append(roc_auc)
    loss_list.append(loss)

print(str(round(np.mean(balanced_accuracy_list),4)) + ' +- ' + str(round(np.std(balanced_accuracy_list),4)))

result = {'accuracy': accuracy_list,
          'balanced_accuracy': balanced_accuracy_list,
          'recall' : recall_list,
          'precision': precision_list,
          'specificity': specificity_list,
          'NPV': NPV_list,
          'F1_score': f1_list,
          'AUC': roc_auc_list
}

os.chdir('/content/drive/MyDrive/Colab Notebooks/GitHub/results')
file_1 = open('./res_lightGBM_realdata.json','w')
json.dump(result,file_1)
file_1.close()