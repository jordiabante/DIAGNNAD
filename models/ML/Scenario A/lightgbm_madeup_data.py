# -*- coding: utf-8 -*-
"""lightGBM_madeup_data.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1l3JlEWf28GmvLzrOO7rYr8Q23oQ0NGxa
"""

from google.colab import drive
drive.mount('/content/drive')

##import library
import gc
import os
os.chdir('/content/drive/MyDrive/Colab Notebooks/GitHub')
import numpy as np
import networkx
import csv
import pandas as pd
from tqdm import tqdm
import gzip
import re
import matplotlib.pyplot as plt
import numpy as np
import tensorflow as tf
from tensorflow.keras.layers import Dense
from tensorflow.keras.models import Sequential
from metrics import metrics
from labelling import label_ADNI
from sklearn.model_selection import train_test_split
from keras import backend as K
from sklearn.metrics import balanced_accuracy_score
from sklearn.model_selection import StratifiedKFold
import itertools
import math
import pickle
import json

import lightgbm as lgb
from sklearn.model_selection import GridSearchCV
from sklearn.metrics import log_loss

# https://realpython.com/logistic-regression-python/

# checkejem que s'obri correctament:
with open('/content/drive/MyDrive/PROJECT/dataset_Age_Sex_madeup_pheno.pickle', 'rb') as handle:
    dataset_list = pickle.load(handle)
dataset_list

# Crea las listas vacías para las métricas
cm_list = []
accuracy_list = []
balanced_accuracy_list = []
precision_list = []
recall_list = []
specificity_list = []
NPV_list = []
f1_list = []
roc_auc_list = []
loss_list = []

for j in dataset_list:
    rows, columns = j.shape
    validation_list = []
    training_list = []

    cm_list_f = []
    accuracy_list_f = []
    balanced_accuracy_list_f = []
    precision_list_f = []
    recall_list_f = []
    specificity_list_f = []
    NPV_list_f = []
    f1_list_f = []
    roc_auc_list_f = []
    loss_list_f = []

    for i in range(5):
        if i == 4:
            train = j.iloc[:int(rows/5*i), :]
            validation = j.iloc[int(rows/5*i):, :]
        else:
            train1 = j.iloc[:int(rows/5*i), :]
            train2 = j.iloc[int(rows/5*(i+1)):, :]
            train = pd.concat([train1, train2], axis=0)
            validation = j.iloc[int(rows/5*i):int(rows/5*(i+1)), :]
        validation_list.append(validation)
        training_list.append(train)

        df_train = training_list[i]
        df_test = validation_list[i]
        print(i, "test SET", len(df_test))

        X_train = df_train.drop(columns=['y'])
        y_train = df_train['y']
        y_train = y_train.astype(float).round(1)

        X_test = df_test.drop(columns=['y'])
        y_test = df_test['y']
        y_test = y_test.astype(float).round(1)

# creem el model d'arbres per classificar
    # Define the parameters for the grid search
        parameters = {
            'num_leaves': [10, 20, 30],  # Test different values for num_leaves
            'min_data_in_leaf': [10, 15, 20],  # Test different values for min_data_in_leaf, numeros grandes para evitar el sobreajuste, al tener un dataset pequeño podría ocurrir
            'max_depth': [None, 1, 5, 10]  # Test different values for max_depth
            }

    # Create the base model
        base_model = lgb.LGBMClassifier()

    # Perform grid search
        grid_search = GridSearchCV(base_model, parameters, cv=5, scoring='balanced_accuracy', n_jobs=-1)
        grid_search.fit(X_train, y_train)

    # Get the best parameters and the best model
        best_params = grid_search.best_params_
        best_model = grid_search.best_estimator_

        # podem calcular el pes de cada predictiu amb la funció següent:
        importances = best_model.feature_importances_
        indices = np.argsort(importances)[::-1]  # Ordenar los índices en orden descendente
        top5_indices = indices[:5]  # Obtener los primeros cinco índices

        print("Los cinco predictores más importantes son:")
        for idx in top5_indices:
            print(X_train.columns[idx])

    # Print the best parameters found
        print("Best parameters found:", best_params)

      # predict the results
        y_pred=best_model.predict(X_test)
        y_pred = y_pred.astype(float).round(1)
        y_proba = best_model.predict_proba(X_test)[:,1] #to obtain the predicted probabilities for each class in a classification problem.


    # Compute the metrics
        classes=['0.0', '1.0']
        cm, accuracy, balanced_accuracy, precision, recall, specificity, NPV, f1, roc_auc, thresholds = metrics(y_test, y_pred, y_proba, classes)
        print("Precisió del model (balanced_accuracy): {:.2f}".format(balanced_accuracy))

    # Compute the loss function
        loss = log_loss(y_test, y_proba, normalize=True, sample_weight=None, labels=[0.0, 1.0])


    # Store the data in the lists
        cm_list_f.append(cm)
        accuracy_list_f.append(accuracy)
        balanced_accuracy_list_f.append(balanced_accuracy)
        precision_list_f.append(precision)
        recall_list_f.append(recall)
        specificity_list_f.append(specificity)
        NPV_list_f.append(NPV)
        f1_list_f.append(f1)
        roc_auc_list_f.append(roc_auc)
        loss_list_f.append(loss)

# Store the data in the lists
    cm_list.append(cm_list_f)
    accuracy_list.append(accuracy_list_f)
    balanced_accuracy_list.append(balanced_accuracy_list_f)
    precision_list.append(precision_list_f)
    recall_list.append(recall_list_f)
    specificity_list.append(specificity_list_f)
    NPV_list.append(NPV_list_f)
    f1_list.append(f1_list_f)
    roc_auc_list.append(roc_auc_list_f)
    loss_list.append(loss_list_f)

print(str(round(np.mean(roc_auc_list),4)) + ' +- ' + str(round(np.std(roc_auc_list),4)))

result = {'accuracy': accuracy_list,
          'balanced_accuracy': balanced_accuracy_list,
          'recall' : recall_list,
          'precision': precision_list,
          'specificity': specificity_list,
          'NPV': NPV_list,
          'F1_score': f1_list,
          'AUC': roc_auc_list
}

os.chdir('/content/drive/MyDrive/Colab Notebooks/GitHub/results')
file_1 = open('./res_lightGBM_madeup_data.json','w')
json.dump(result,file_1)
file_1.close()