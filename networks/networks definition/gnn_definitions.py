# -*- coding: utf-8 -*-
"""GNN_definitions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1u2sN4UauWmAs6fNsxh6CbimNxc2SxyNa
"""

from sklearn.metrics import balanced_accuracy_score

import torch
import torch.nn.functional as F
from torch.nn import Linear

from torch_geometric.datasets import TUDataset
from torch_geometric.loader import DataLoader
from torch_geometric.nn import DenseGraphConv, DMoNPooling, GCNConv, TopKPooling, global_mean_pool, GATv2Conv
import torch.nn as nn
from torch_geometric.utils import to_dense_adj, to_dense_batch
import matplotlib.pyplot as plt
import os
import sys

sys.path.append('/biofisica/home/creatio_student/project_gnn_ad/file_articolo/')
from metrics import metrics

class GCNNet(torch.nn.Module):
    def __init__(self, num_features, hidden_channels, num_classes, out_channels, num_layer):
        super(GCNNet, self).__init__()

        # Define a list to store the GNN layers
        self.gnn_layers = torch.nn.ModuleList()

        # Add the first GCN layer
        self.gnn_layers.append(GCNConv(num_features, hidden_channels))

        # Add additional GCN layers (if num_layer > 1)
        for _ in range(num_layer - 1):
            self.gnn_layers.append(GCNConv(hidden_channels, hidden_channels))

        # Define the first graph pooling layer
        self.pool1 = TopKPooling(hidden_channels, ratio=0.8)

        # Define two fully connected linear layers for classification
        self.lin1 = torch.nn.Linear(hidden_channels + 2, hidden_channels + 2)
        self.lin2 = torch.nn.Linear(hidden_channels + 2, out_channels)

    def forward(self, x, edge_index, batch, AGE, PTGENDER):
        # Forward pass through the GNN layers
        for conv in self.gnn_layers:
            x = F.relu(conv(x, edge_index))

        # Apply the first graph pooling layer
        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, batch = batch)

        # Compute the global mean pooling of the node embeddings, based on the batch index
        x = global_mean_pool(x, batch)

        # Concatenate AGE and PTGENDER with node embeddings
        x = torch.cat([x, AGE.unsqueeze(1), PTGENDER.unsqueeze(1)], axis=1)
        x_toprint_1 = x.clone()

        # Apply two fully connected linear layers for classification, with ReLU activation in between
        x = F.relu(self.lin1(x))
        x_toprint_2 = x.clone()

        x = self.lin2(x).squeeze(1).float()

        # Apply a sigmoid activation function to the output logits and return
        return x, x_toprint_1, x_toprint_2

class GAT(torch.nn.Module):
    def __init__(self, num_features, hidden_channels, num_classes, out_channels, num_layer):
        super(GAT, self).__init__()

        # Define a list to store the GNN layers
        self.gnn_layers = torch.nn.ModuleList()

        # Add the first GCN layer
        self.gnn_layers.append(GATv2Conv(num_features, out_channels, heads = hidden_channels, concat = True, negative_slope = 0.2, dropout = 0.0))

        # Add additional GCN layers (if num_layer > 1)
        for _ in range(num_layer - 1):
            self.gnn_layers.append(GATv2Conv(hidden_channels, out_channels, heads = hidden_channels, concat = True, negative_slope = 0.2, dropout = 0.0))

        # Define the first graph pooling layer
        self.pool1 = TopKPooling(hidden_channels, ratio=0.8)

        # Define two fully connected linear layers for classification
        self.lin1 = torch.nn.Linear(hidden_channels + 2, hidden_channels + 2)
        self.lin2 = torch.nn.Linear(hidden_channels + 2, out_channels)

    def forward(self, x, edge_index, batch, AGE, PTGENDER):
        # Forward pass through the GNN layers
        for conv in self.gnn_layers:
            x = F.relu(conv(x, edge_index))

        # Apply the first graph pooling layer
        x, edge_index, _, batch, _, _ = self.pool1(x, edge_index, batch = batch)

        # Compute the global mean pooling of the node embeddings, based on the batch index
        x = global_mean_pool(x, batch)

        # Concatenate AGE and PTGENDER with node embeddings
        x = torch.cat([x, AGE.unsqueeze(1), PTGENDER.unsqueeze(1)], axis=1)
        x_toprint_1 = x.clone()

        # Apply two fully connected linear layers for classification, with ReLU activation in between
        x = F.relu(self.lin1(x))
        x_toprint_2 = x.clone()

        x = self.lin2(x).squeeze(1).float()

        # Apply a sigmoid activation function to the output logits and return
        return x, x_toprint_1, x_toprint_2

def train(train_loader, model, device, optimizer, scheduler):
    # Set model to training mode
    model.train()
    # Initialize loss accumulator
    loss_all = 0
    y_train = []
    y_age = []

    # Loop over each batch in the training data
    for data in train_loader:
        # Move data to device (e.g., GPU)
        data = data.to(device)
        # Reset optimizer gradients
        optimizer.zero_grad()
        # Perform forward pass of model on the current batch
        pred, _, _ = model(data.x, data.edge_index, data.batch, data.AGE, data.PTGENDER)
        #print(x_full_test)
        #print(pred)
        # Compute loss for the current batch
        criterion = nn.BCEWithLogitsLoss()
        loss = criterion(pred, data.y)
        # Perform backward pass to compute gradients
        loss.backward()
        # Accumulate loss for this batch
        loss_all += data.y.size(0) * float(loss)
        # Update model parameters using optimizer
        optimizer.step()
        # use the scheduler
        scheduler.step()
        y_train.extend(data.y.cpu().detach().numpy())
        y_age.extend(data.AGE.cpu().detach().numpy())

    # Return average loss over all batches in the training data
    return loss_all / len(train_loader.dataset)

@torch.no_grad()
def validation(loader, model, device):
    # Put the model in evaluation mode
    model.eval()

    # Initialize variables to keep track of metrics
    correct = 0
    loss_all = 0
    y_pred = []
    y_proba = []
    y_val = []

    # Loop through the validation data loader
    for data in loader:
        data = data.to(device)
        # Get predictions from the model
        pred , x_2, _= model(data.x, data.edge_index, data.batch, data.AGE, data.PTGENDER)
        #print(pred)
        # Calculate the loss
        criterion = nn.BCEWithLogitsLoss()
        loss = criterion(pred, data.y)
        loss_all += data.y.size(0) * float(loss)
        probabilities = torch.sigmoid(pred)
        # Convert the predictions to binary values and add them to the list
        y_pred.extend((probabilities > 0.5).to(torch.int).cpu().detach().numpy())
        # Add the predicted probabilities to the list
        y_proba.extend(probabilities.cpu().detach().numpy())
        # Add the ground truth labels to the list
        y_val.extend(data.y.cpu().detach().numpy())

    # Calculate the balanced accuracy of the predictions
    balanced_accuracy = balanced_accuracy_score(y_val,y_pred)

    # Return the average loss and balanced accuracy
    return loss_all / len(loader.dataset), balanced_accuracy

@torch.no_grad()
def test(loader, model, device):
    # Switch model to evaluation mode
    model.eval()

    # Initialize variables for tracking metrics
    loss_all = 0
    y_pred = []
    y_proba = []
    y_test = []
    y_age_test = []
    x_full_test_post_GNN = []
    x_full_test_post_lin = []
    concatenated_list_post_GNN = []
    concatenated_list_post_lin = []

    # Loop over batches in the data loader
    for data in loader:
        # Move data to GPU if available
        data = data.to(device)
        # Make predictions with the model
        pred, x_post_GNN, x_post_lin = model(data.x, data.edge_index, data.batch, data.AGE, data.PTGENDER)
        x_full_test_post_GNN.extend(x_post_GNN)
        x_full_test_post_lin.extend(x_post_lin)
        #print(x_full_test)
        #print(len(x_full_test))
        #print(pred)
        # Calculate loss with binary cross-entropy loss
        criterion = nn.BCEWithLogitsLoss()
        loss = criterion(pred, data.y)
        # Update loss
        probabilities = torch.sigmoid(pred)
        loss_all += data.y.size(0) * float(loss)
        # Append predictions, predicted probabilities, and true labels to lists
        y_pred.extend((probabilities > 0.5).to(torch.int).cpu().detach().numpy())
        y_proba.extend(probabilities.cpu().detach().numpy())
        y_test.extend(data.y.cpu().detach().numpy())
        y_age_test.extend(data.AGE.cpu().detach().numpy())

    # Define class labels
    classes=['0', '1']
    # Compute metrics with the predicted and true labels
    cm, accuracy, balanced_accuracy, precision, recall, specificity, NPV, f1, roc_auc, thresholds = metrics(y_test, y_pred, y_proba, classes, save_plots = True)

    for i, j, k in zip(x_full_test_post_GNN, y_test, y_age_test):
      concatenated_tensor = torch.cat((i.clone(), torch.tensor([j], dtype=torch.float32).to(device), torch.tensor([k], dtype=torch.float32).to(device)))
      concatenated_list_post_GNN.append(concatenated_tensor)

    for i, j, k in zip(x_full_test_post_lin, y_test, y_age_test):
      concatenated_tensor = torch.cat((i.clone(), torch.tensor([j], dtype=torch.float32).to(device), torch.tensor([k], dtype=torch.float32).to(device)))
      concatenated_list_post_lin.append(concatenated_tensor)


    # Return average loss and computed metrics
    return loss_all / len(loader.dataset), cm, accuracy, balanced_accuracy, precision, recall, specificity, NPV, f1, roc_auc, thresholds, concatenated_list_post_GNN, concatenated_list_post_lin